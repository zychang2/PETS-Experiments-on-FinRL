{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /scratch/961756/pip-req-build-rpk1b7ev\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /scratch/961756/pip-req-build-rpk1b7ev\n",
      "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 0496601ee2824ca675beb5a220afd9109ac02ffa\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl (from finrl==0.3.6)\n",
      "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /scratch/961756/pip-install-akulojts/elegantrl_8ed5e98f669c488bbcbb6f9fe65f261b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /scratch/961756/pip-install-akulojts/elegantrl_8ed5e98f669c488bbcbb6f9fe65f261b\n",
      "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit c2939fefe0e3ec55601ded49e39fdf9d7d781ea0\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: alpaca-trade-api<4,>=3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (3.2.0)\n",
      "Requirement already satisfied: ccxt<4,>=3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (3.1.60)\n",
      "Requirement already satisfied: exchange-calendars<5,>=4 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (4.5.8)\n",
      "Requirement already satisfied: jqdatasdk<2,>=1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (1.9.6)\n",
      "Requirement already satisfied: pyfolio<0.10,>=0.9 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (0.9.2)\n",
      "Requirement already satisfied: pyportfolioopt<2,>=1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (1.5.5)\n",
      "Requirement already satisfied: ray<3,>=2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.39.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (1.5.2)\n",
      "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.5.0a0)\n",
      "Requirement already satisfied: stockstats<0.6,>=0.5 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (0.5.4)\n",
      "Requirement already satisfied: wrds<4,>=3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (3.2.0)\n",
      "Requirement already satisfied: yfinance<0.3,>=0.2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from finrl==0.3.6) (0.2.50)\n",
      "Requirement already satisfied: pandas>=0.18.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.32.3)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.20)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.8.0)\n",
      "Requirement already satisfied: websockets<11,>=9.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (10.4)\n",
      "Requirement already satisfied: msgpack==1.0.3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.0.3)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.11.7)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0.1)\n",
      "Requirement already satisfied: deprecation==2.1.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: packaging in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.2)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (75.1.0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (2024.8.30)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (43.0.3)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (3.2.0)\n",
      "Requirement already satisfied: yarl>=1.7.2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (1.18.0)\n",
      "Requirement already satisfied: pyluach in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.2.0)\n",
      "Requirement already satisfied: toolz in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (1.0.0)\n",
      "Requirement already satisfied: tzdata in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2024.2)\n",
      "Requirement already satisfied: korean-lunar-calendar in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.3.1)\n",
      "Requirement already satisfied: six in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.16.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.8 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (2.0.36)\n",
      "Requirement already satisfied: pymysql>=0.7.6 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.1.1)\n",
      "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (0.5.2)\n",
      "Requirement already satisfied: ipython>=3.2.3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (8.29.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.9.2)\n",
      "Requirement already satisfied: pytz>=2014.10 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2024.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.12.0)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.13.2)\n",
      "Requirement already satisfied: empyrical>=0.5.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.5.5)\n",
      "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.6.0)\n",
      "Requirement already satisfied: click>=7.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.7)\n",
      "Requirement already satisfied: filelock in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (3.13.1)\n",
      "Requirement already satisfied: jsonschema in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (4.23.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (4.25.5)\n",
      "Requirement already satisfied: aiosignal in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (1.5.0)\n",
      "Requirement already satisfied: aiohttp-cors in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
      "Requirement already satisfied: colorful in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.5.6)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.0)\n",
      "Requirement already satisfied: opencensus in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.11.4)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.10.0)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.21.0)\n",
      "Requirement already satisfied: smart-open in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (7.0.5)\n",
      "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (20.27.1)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.67.0)\n",
      "Requirement already satisfied: memray in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.14.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.6.2.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (18.0.0)\n",
      "Requirement already satisfied: fsspec in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2024.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.5.0)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.0.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.3.0.post101)\n",
      "Requirement already satisfied: cloudpickle in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.0)\n",
      "Requirement already satisfied: opencv-python in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.10.0.84)\n",
      "Requirement already satisfied: pygame in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.3.dev8)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.17.1)\n",
      "Requirement already satisfied: psutil in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (6.1.0)\n",
      "Requirement already satisfied: tqdm in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.66.6)\n",
      "Requirement already satisfied: rich in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.9.2)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.10.1)\n",
      "Requirement already satisfied: pillow in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.0.0)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from wrds<4,>=3->finrl==0.3.6) (2.9.10)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (5.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.3.6)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (3.17.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.1)\n",
      "Requirement already satisfied: th in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.4.1)\n",
      "Requirement already satisfied: pycares>=4.0.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6) (4.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.4.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ale-py>=0.9.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.6)\n",
      "Requirement already satisfied: cffi>=1.12 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.17.1)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.7.post3)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.9.0)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.7)\n",
      "Requirement already satisfied: pandas-datareader>=0.2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.0.4)\n",
      "Requirement already satisfied: webencodings in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6) (0.5.1)\n",
      "Requirement already satisfied: decorator in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6) (2.27.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (3.1.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.4)\n",
      "Requirement already satisfied: Cython>=3.0.10 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.0.11)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.11)\n",
      "Requirement already satisfied: sympy in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.13.2)\n",
      "Requirement already satisfied: networkx in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.3)\n",
      "Requirement already satisfied: jinja2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.9)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.21.0)\n",
      "Requirement already satisfied: textual>=0.41.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from memray->ray[default,tune]<3,>=2->finrl==0.3.6) (0.86.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.23.0)\n",
      "Requirement already satisfied: wrapt in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from smart-open->ray[default,tune]<3,>=2->finrl==0.3.6) (1.16.0)\n",
      "Requirement already satisfied: niltype<2.0,>=0.3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (1.0.2)\n",
      "Requirement already satisfied: pycparser in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.22)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.66.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.25.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.36.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/hice1/skamath36/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
      "Requirement already satisfied: qdldl in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from osqp>=0.6.2->cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7.post4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from sympy->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (4.9)\n",
      "Requirement already satisfied: mdit-py-plugins in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default,tune]<3,>=2->finrl==0.3.6) (2.0.3)\n",
      "Requirement already satisfied: uc-micro-py in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default,tune]<3,>=2->finrl==0.3.6) (1.0.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /storage/ice1/9/3/skamath36/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 12:47:43.398362: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-21 12:47:43.404165: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-21 12:47:43.416302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-21 12:47:43.437465: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-21 12:47:43.443851: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-21 12:47:43.459076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 12:47:45.599092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjoAAADICAYAAADhjUv7AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAB3RJTUUH4gkMBTseEOjdUAAAHzdJREFUeNrt3X+sXWW95/H31zSZ/tFkesdOpnM9wU5bM72ZGkosCnKq4K20zJRRIsZThVgyIhZhIlEKXjE4USNFHXJD6EHQ2IlIa6gBB2Y4hSo/eu4VpV5q7A1MPK3Vqdqb4Tqd3P7BH02+88d6dlld7NOe32f/eL+SnXPO/rHO2s9a+3k++3metVZkJpIkSb3oTRaBJEky6EiSJBl0JEmSDDqSJEkGHUmSJIOOJElSzQKLQOocEbEYuAY4H1gLrPZz2pFOAYeAA8Avgd2Z+arFInVgvep5dKSOCTmbgGFgFPgb4AXgYGaesnQ6blstKCF0LXAJsBG4OTP3WDqSQUfSGxvOrwObgOszc9QS6brtd1EJqQcy83pLROocztGR5r+R3FRCzoWGnO6UmS8AFwJrI2LIEpE6qI61R0ea15CzGPgVsNmQ0xPbcw3wNHBBZh6zRKT5Z4+ONL+uAUYNOb0hMw8CewB7dSSDjiTgXcBei6Gn/LhsV0kGHanvraU6ukq94yCwxmKQOoNzdKT5/ABGZGaGJeF2lTQ77NGRJEkGHUmSJIOOJEmSQUeSJMmgI0mSZNCRJEky6EiSJIOOJEmSQUdSX4iIwYjIiPBMo5IMOpJ6zkcp1+aKiHm7cGUtcA26SSSdzQKLQNIkbAXWld+3ALstEkmdzB4daQ5ExOIeeA9DwOHMHAV2AhsiYnmb52Wb21jt8cHxHiuP74iIkYgYajxveetxYH95+v7y2A73MkkGHWn+fCYiXoyIqyOiW3tStwBPld9/Xn5e3QgpY8BwZka5qOVeYG9mrqyFpf3AitpzxpphB9gAbGks5ymAzLyR13uV1pXn3OguJsmgI82f48Ba4BHg5Yi4KSIWdsvKl96UDcDDJWwcKeHjk43nrGg9p9hZXtfyFeC28vr6fSsa820OZ+bGxnJWtOtBkqSzcY6ONP0QsAAYAJbVfr4FWAgsARYBS2svWQncC3y2i97m1SXgjDbCx66IGMzM0cw8EhGHqSYst563pQSilhXA9ojYPsn/f6z8/HPgSJfsF78pv75Wgm7L0UYA/n257xhwLDNf9VMlGXSkuW60FgKrgTXAv62Fmtat1VC1fv49cBI4UW4bgNvL4k6VkPBF4I9dUgSfLOXQ7rDyerAB2BoRW8vvh1vDVjW3Zebdvb7PZOa/iYiBWj3bCr2Un0tKGH4r8K5WSI6IJa3QU/an3wOHgEOZ+YqfRsmgI0031CwqgWYtcH75fTXwCnCwhJhf1L6BH51gULodGAFuzcxD5f5uKI9Bqp6YdY0endbE4K3Aja3nlTk14zlcQuJ0/aFLws6x2p9HJ1HmA40w/QHgCxGxrISeA2U/PFAC0Ck/uZJBRxqvUVkGbATeW8LNQGlMDpZAcx/wSmaenMa/OQq8PzP3dWERfZTXj7Zqep6qB2ewFT7a9Prsrc23eYBq6Or5zNxdnr8ceKpNz89EvJsze5N6QglIx8YJzKtrIfwGYGVEHAVeAJ4Dns3M436ypfK5yfQEp+q7YLMUuJRqOOlSquGDZ4Efz/U35IjIc/SAdEJ5JWcZbiqPD2fmjeX3M3p+yhFVT7WOjCpHXu1qNOxRe/4O4PJ68ClBan992RGxDWjN9emo4bC53q4RsRoYLGH9UuBVYF8t+Jzwky+DjtS7wWYhsL4Em/VUPTatYPNsZh7slwZxlt/LGwJK7f7ljaOoen2fm9ftGhFrSuD5yxKAXmns8w51yaAj9UC42Qh8CNhENQz1HNUcmQOdUtH3WNBp9bCsaB0+XoalDtMnE5A7cbuWowLXls/DXwKrgMeAHxh6ZNCRuqtxWVAq84+UcHOgVOaPdeohu70UdMr7aU1Ortvcmo9j0OmIdRugOl3Ah0ro2Q38YJw5WJJBR+qAint9CTcfpOqi/yHwUDecj6TXgo66a7uW0LOlhJ4lwJ4Sel5wK8qgI81vBb0Y+ATVUScngO8DexqH89ogyu068XVeCQwBH6c6B9R95QvDa25RGXSkuauMVwOfLhXyE8B93fzt06Bj0OnQ9d9UvkQMAt8un7Ojbll1I691pW6odBeUi2HuBx4Hfgu8LTOvtYtdmnmZ+URmXglcSHW+tZci4vESgKTuakPs0VEHB5zFwE3lm+Uhqq70kV46SsQenZ7dd3ttkvlCqrk8n6Y679R9wP0Oa6kb2KOjjgw4EfEl4NdUlx64LDOvKN8yPRRWmmOZ+Vpm3p+Zb6c6qu4DwG8i4jMlBEkGHWmSAeetwMWZeV1mjlk6UseEnn2ZeRmw2cAjg45kwJF6NfA8a+CRQUc6e8BZGBF39HnAGSuH9ap39uuVtLkgZ58FnpvKCTwlg476tjHYCPwKeAf93YNzgOoQXvWONWW79pU2geelcjFWyaCjvgo4yyLiUeBe4ObMvKrPh6h+RnXFafWOS4Bf9OubL4Hn/cBXgV0R8b2IWOpuIYOOej3gtIapXiqNwNszc8SSYTewMSIusih6Yj9fBVwDPNTvZVGub/Y24ChV785nHM6SQUe9WvnXh6kuyMyveP6N043BceBmYNhGoOv38wXAd4HPexbh0/v3a5n5RWAdsAGHszQfn01PGKhZrPgXUw1RXUQ1TGUPzvhl9SCwFrguMw9aIl23/VaVkDOWmddaIuOW0weBe4B9wC2ZedJS0WyzR0ezVaFdSjVMdQKHqSbyzfd6YDvwdETcWy554dFYnb2Pryzb6R5gP/AdQ8459/PHgLcDp6h6dxyy1ex/Vu3R0QxX/guArwFXA9dn5j5LZVLlN0B1qv13UB2NtcRS6VgngFGqOWc7Ha6a9L6+CXiQ6nISd3nWcxl01A0V12rge8BYCTknLBVJZ6kzlpawswTYbFjUbHDoSjNVYX0GeBr4ZmZ+2JAj6Vwy83i5Svr3gRcjYoulohlvn+zR0TQDziKqXpzFwLWZecxSkTSFumQVsAs4SNUj7FCWZoQ9OppOxTRANQnzOPB+Q46kqcrMV6gOQ18CPONJBmXQ0XyHnIuAnwLfysytfvuSNANh5yRwFdUk75+WeX/S9Norh640hZBzDfB1qqEqj6qSNBv1zBaqc+5cm5lPWCKaKs/EqslWPl+mOnT8stLVLEkzLjN3RsQY1fWyVmfmXZaKptRu2aOjCQacBVQTBRcDHlUlaa7qnmXA48C+zLzFEpFBR7MZcpYCV3jadklzXActAp4EDhh2NFlORpYhR1JHK/XOFcDacskNyaAjQ44kw45k0JEhR5JhRwYd9R1DjiTDjgw66j0R8SVgwJAjqcPDzqURcbslorPxPDpqhpwPAjcAFxhyJHVy2ImIq4D9EXEwM0csFbVt1zy8XLWQs5rq2lVXZOYLloikLqi3LgUeBS72JKZqx6ErtSqLxaWyuNWQI6lbZOazwK3Ao6Uek85s3+zRUTnC6nHgaGZutUQkdWE99iDV3MIrvciw6uzREcCXgYXAzRaFpC61tdRjX7ModEYItken778FXQQ8AlyYmcctEUldXJ8tBV4ENmfmqCUisEen3yuFBcCDwC2GHEndrtRjNwMPRsRCS0QGHd1ONS9nj0UhqUfCzmPAoVK/SQ5d9e2Gj1hFdSj5BZl5zBKR1EP12wDwErDOQ85lj07/ehD4L4YcSb2m1GtfBL5bhuhl0FGffdv5FNVZse+3NCT1aNi5HzgFfMLSMOioO8PKWETsmMLrFgBfoDoxoOeakNTLbgbu7JaJyRExGBEZEUNuug4JOhGxo2yU7IaNExHLG+vbum3ro20+RDUB2UMvJfW0zDwIvFLqvdloU1ptyKCl3YNBp/QmbM3MaN2Ar0TE8kaoGJrkcpfPQWjaXFvnzcD2Pgo7nwW+6a4vqU98s9R7Mx1yhoDD5fbRKbx+JCJGGsFstLRNu91sHRB0gMuB4cZGWpmZR7os8e8uO+r7en1jR8R6YFE5/FKSel5mPlHqv40zvOgtwAPl5qVzejTotMJOuwZ1WwkPALtKD81I7fGxxtDR4ARfN9R43cgshoKR8Ybl2g13lfc00rhvR0SMTXCZrZ6sweaQWuO+6fR2fRbY7m4vqc/MaK9OGbnYAOwpN9rVy23arG2tur68fkPtseXjjWi0aTt2tGlrRtr8v+Vu+irtTulGNeaZ5TbU5vHl7R4DxoDB2t/bqtU45+vOeF5tWSOTWOc3LLu13MZ9Y8CO2t+D9ecAO4CxxnJHxlm/beX3kcb/aJXf8sa6ZaN8Wvdvayw36+s4gfe+BvgjsHCq29ybN2/euvFGdQ2sPwJrZmh52xptwBvaolodP9h43vJamzAygTZqrP6/yn1Zf21pk5r3jTRf16+3N00jIO0uc1zqvS9DE3jdysZE2L+tJeSz2U41n6bujpKIJ5taW+ubwPb6mGh5Dysy88baOo8Ce0tXJcDzwIra/30n8BNgb613ahBY0Xp/mbmxMe768/LzzxvrdlujfK4ur7+7Xoa1nq+J+gjwrcx8zXgvqc++0L8G3Ad8bIYW+UmqIauWB9q0RV8Bhuv1+WSnd7TaozajJ5vb/L/DmVkfntvZaKccuprGDtSa1Hu4BIihCWy8rAWN/eM0+M1uwjMCSnntrimu9uayzita3X61x85rrmOtm/F0yKsFHID3lEDzE+Dd5b53lx1vtDG81VpeK6gMNNbtd42/31dC1nStLwlfkvrRCDDteTrNL7HFnvoX02IFcHSa/+680uY0w9Gxc7WbE3yOQWeSgafVy7DlbIGlNPLDtYC0brIBpc3tyBTX+QhwG7C1mXrH+T/1D8lw7b1uLYHmb0vSbwWUB+rhjqobMeohay6UK/quAg5Y10nq016dA8DSUh9OR+sIq/1tvrh+0pLu4aBTc2ScBFpPlt84R/gY777zZmHnbw0Jfa78/F0rlJ3jpc9TdR0OtnbyEnZW1CaqNYflvjLF8lzZ5v7JBKX1wD5PECipz+1j+r06W6mmGETj9CqbS/3fOqfOYWDZudrKcxivPWqNBPzBTTpLQad1FFDjvm2l8X248fT31H5vbZR6997+cf7Nexp/D1Od72awsR71o7K2TXGm+XDZeevDUk813t+O+rBc7Xl3NJ67l2pi2Olhq1pQq59r4akJrtvD5cOzrbYuY5N8fxuYmeEvSepme4H3TvXFtTZgT5uHW/MuW9MXHqAaLai3WWON9mnDOb6It9qZHbVlLKeatjHcbadz6aqgUxrwzY05LNupJvHWJ9JuLhs6I2JH2SitE/S1Xre5zb8443Xlf95INcxU7y7c2RhOmqqH6ztxa5J14/0dbXMSp71lR32+dt9Pyn0PNJ67rvaesgSkCZd1o8y2TDK4rC/fZCTJHp2p2wLsPcvIw17K8FUZLWi2WQ+0Xts64KX22HhtQAArG8Nkt9UPmNE5Amo5DK033kzp3Zmh8NMrZbIS2J+Z/9rSkGSdGL8CrszMo5ZGf1jQQzvvIFVPygo36xkGqM7DIEmqjkZaxvSPiFKX6KWrl3+UqjvPMcs3Bp3jFoMkQakPl1kM/aNnenQcrxzXEoOOJJ32W2CpxdA/3mQR9Lx/BfyDxSBJUL74vcViMOiodyzl9TNkSpJB541npJdBR11smUFHkk47ASy2GAw66h2vUs3TkSTJoKOecxwn3klSywAeWm7QUU/5B6oJyZIkj0Q16KjnHMMeHUlqeTMeiWrQUc8FnWUWgyQB1dDVqxaDQaerRMSby9XFWxfh/FPrYqBTXN5gucrsn8rvyyNid1n27i4rHufoSNLrOuaUGxFxQ2lrMiJejIihLmxjOl6vnBn5PqprXC2p/X3hFHe85VSXk3hXSf03UR2K+LHy85kuK5sxYCAiFmfmCXd5SX1uLfBKB4ScrwJ/BWzOzN0RMQTsAobdRDNc1r1w9fKI+BNwV2beXf4eBG7KzKFpLjeBw8C7MvMfu7h8ngT+W2b6TUFS/zZ4ERcB92bmhfO8HoPAfuBTmfmtRpuz2bp6ZvXKHJ2ngNsj4nyAzBydgZBzfvn1jm4OOcX/oLqyuyT1s/XAvg5Yj48C/7cRcgbLry+7mQw67fwVVc/LMxFxQ5vQcsMU5uxcVH4+PUPLm08j5QMuSf1sA7C3A9ZjqHxBr/t3Jfz8stHetIa11M9BJzOPABuBu4D7y9hn3VVM/gRR5wMHxunNmcry5rN8xoDXImK1u7ykfhQRi4HVwGgHrM6fAX/XuO9W4OeNdX4zcDlexqe/g05EvFga838sc3SGgXc0Ht8AbC8z27dNcNGXAy+O8/+msrz5NgJscpeX1KfWA6OZeaoD27EbgH8B/KR23/nAz0oo2l/am0E3Y58FnbIjrG1165Ujpi6kumhby0fKzyWZGa0Jy+X5bYNKSdErgF+2+bfjLq/D/QD4uLu8pD71MeBHHbIuh4H3lfZmCDiv1v4MRsRXyxDWHVQjC1Fuo27GPgs6wD+VBnxHma1+gKoX5tO157yT8YegxvMX5efft3lsKsubd+UD8lpEfNDdXlI/iYiVwCDwUIes0n8G3lmOGD4vM79ANWdnO3AF8F/L897DG+fyaLLbvxcOLz/HDr4b+Ltmz0vpAvzvwNoyx2day+uSsrgG+E+ZeZm7vqQ+CjrDwKuZ+cUuW+8/Af/Rnpzp6YdLQKwFflfOblw/IusO4LLJhJxzLK8b7AZWRsRad31JfRJyllAd5XRfl633cqr5OX8ow1n/3q1p0BnPD6jONrkD2NO6MzM3Ng/jm87yukGZhPfXwG3u+pL6xE3AY5nZVVcsL1/CD1DN57kiM/+nm3KKobHXh670hm8Ji4FfAxeXw84lqVfru4XAb0pQOGiJ9CevXt5nyvWudmKvjqTe9yngkCGnzwOvPTp9+S1nEdVpxq/NzGctEUk9WM8NAC8B6zLzFUukf9mj04cy8ySwFRguXbuS1GvuAe4z5Mig079h5wngEPAFS0NSL4mITVSXe7jL0pBDV/1dGSwFfkV1mP0hS0RSD9Rri0q9dp1D8wJ7dPpaOdzyVuDBiFhgiUjqAXcCzxpyZNBRK+zsBE4Ct1sakrpZGbIaKl/gJAD8Fi+Aa4GfRsShzHzM4pDUhSFnFfA94KrMfNUS0el9wzk6KpXEauBpPLGWpO6rvxZRXdD5m5n5bUtEBh2NV1lsAu6lOmvycUtEUhfUWwuAR4HjmXm9JaImh650WmY+ERFrgEci4rJybSxJ6mR3AkuAqywKtQ3D9uiozTek7wGnMvM6S0NSB9dVV1OdGPBCe6Fl0NFkKo+FVPN1DmTmLZaIpA6spwaBR4ArM/OAJaLxeHi53iAzXwOuANZGxD2WiKQODTkfNuTonPuLPTo6S2WyCHgSe3YkdU69tKbUSx/OzFFLROdij47GVS7+ac+OpE4JOauAx6ku72DIkUFHhh1JPRVyngZuycwRS0QGHc1W2Bn2uliS5jjkDNZCzh5LRAYdzWbYWQo8GRFLLBVJcxBytlANV2015Migo1kPO5l5FfA3VNfGWmWpSJrFkPNlqhMCrsvMJywRTWk/8qgrTbECGqI6Udf1VkCSZrh+WUR1gc6lVBfp9GSAmjJ7dDQlmbmbaijr3oi43RKRNEMhZymwHzgJXGbIkUFH8xl2DgIXAx+IiEectyNpmiFnDfAS8KPMvLacvFQy6Ghew85xYB1wHHgpIjZaKpKmEHI+R3UiwJsz80uWiGZs33KOjmawotoIPAg8BtzqtzFJE6g3Bqjm4wBcm5nHLBUZdNTJldaSEnZWAZvL8JYmXn6LgWuA84G1wGrA8xZ1nlPAIeAA8Etgd2a+arFMen8fAu4FtmfmNywRGXTUTRXYJ4CvA9uBb2TmKUvlnGW2CRgGRqkO4X8BOGjZdeS2WlBC6FrgEmAj1ZCL53mZeKC/F1hD1YvjFyIZdNSVldkyYFf59ntdZo5ZKuOW1deBTVSH63sNn+7bfheVkHogM6+3RM5aVut5fYj78w5xa7Y5GVmzJjOPUk1U/hHVCQa/Vs6PoTMr/k0l5FxoyOnaff0F4EKqy6QMWSJt9/OBiNhVAuF1mXmLIUcGHfVCA3CqjL2/HRgAXrYhOKPyX1wq/uvLZTbUxfs6cB3VuaUGLJHT+/iCiPgM1WHj/wt4e2Y+a8lozvZBh640x5XeINXY/Emqa9cc6vPyuAm4JDM3u3f0zDYdBg47ufb0530YOEY1h8nha805e3Q01996R6m6+L8PPFOuhr64j4vkXcBe94ye8uOyXfs54CyJiAep5uh9NTOvMOTIoKN+CjunMvN+4G3lrl9HxJf6NPCspTq6Sr3jINXRRP0YcBZHxJeAl6l6bf+iXC5GMuioLwPPiczcSnUZibf2aeBZlZmvuDf01H49Bqzs04Dz6/JZvrhMNnbemQw6UmaOZeZ1tcDzch/38EjdHnA8lYQMOtI5As8FwD838EgGHMmgo14MPMcz85Za4Pl1RNwbEastHWleA86qiLjHgCODjjSzgedtwG+BRyPimYi4upyCX9Lsh5sFEbEpIp4Efkp1pnMDjrpnH/Y8OuqySncj8Gmqo1q+A9yfmce7+P1kZoZbtuf2067frmXI+BPl83YCuA94yLMZq9vYo6OukpkjmXkl1aUl/hnwUkQ8Ur5xLrSEpGkHnDXlHDi/Ad4BbM7MCzLz24YcdeU+bY+OurxSXghcDXyc6pw0TwA/BEa6oVK2R6dn98uu2q5l/ttHgNblWb4D7Ozm3lLJoKNebFyWANcAH6Ia2nqs00OPQcegM4/ruLIEmw8BS4CdwA8z86BbUAYdqfMbmgGqnp6PAKtL6PkB8GwnncTMoGPQmeP1WgVsLJ+LAWBPCTejbjUZdKTubXRa31z/A1VPz0Gq60s9C7wwn709XfLNfzlwGLgtM+92j+qe7RoRS4FLgQ3lJ8C+Wug/5daSQUfqrQZoETBYq/hXAqPAc8C+zDzQTQ1iROwAtrZ5aDgzbzTo9FfQabN/D5Rg8xzVEO5Rt44MOlJ/NUiLqbry31sahqVUPT4HgV8Ah4BDs/XNd4aCzuWZudKtOePbZgx4aiqB8WzbNSIWZ+aJGVi/hcAqqkn45wMXleD+AtUV1Pc530YCT7qmvlYanN3l1urqX0s1r+cDwJ3AQEQcKuHnl+XnoZlorNRXwelS4B7gr6km/k7mtYuohl3XlFCzFlgGvAIcKPvld2YzlEvdyvPoSGcGn+OZ+URm3pWZH87MtwH/ErilNCbnA/cC/zsi/ikiXo6IJyPiwXJdri0RcWlErOyE8/pExPKIyIgYjIix8nuWnqD640ON1w22XtfqoYiIbfUei4gYqi1zR70npPZ/znhdeXwkInZExLb68xrPGSr3L28sq74+2W7d2zyeZfjtXMseqr93YAWwtd36TXIbrI6Ix4FnSlBp95yBiLiorNvnyiVPHo2IlyLi/1BdcuFOqssuPAdcm5l/lpkXZ+bN5Rw3Bw05kj060lTCz0mqeTyjjcZpMdUciIHy7fotVENgHy9/D5RLVbwKtI70Oln+hupU+kTEBzPzsVl+G/uBdZk5WsLC/oh4PjN3R8ReYAulV6t4N3D4HEfj7KI6mdzuesAA9raG0lrzeyJiWWMIaCvVPKKohaORzNzY+B+Ha8/ZUdab2nvZBuyKiJ9n5pHafKLT61WeczgiVmTmkXGWXV/OaHXX1IeuyjKXAl+jOuVBva79ekTcWft7CXCs3I4Cv6caNv1R+fuYJ+qTDDrSfASgE1Snxj90jgZvCbCo/LmoNGytz9/60phNx4pmj0Ob+SGbW6GlBITDwHtKuNlZGvnltSDwSeCBc/zf4UbI2VaWv7G2Hkci4jZgO1APDHsbAeKB8pw3vLfa7w+XgLSuFsD2lNe9EzgCfK4se3dtHe6OiO1Upxu4e5xlN5czE06U3pc1jZ6ch6iGr05m5qt+kqTZ5dCVNPuB6NXMPFpuhzLz2XLbVx6f7oTRw5kZ9dsEXjMGLC//vxUK3lnrhVlRGv+zaQa0ZaU3pel3teWOZyLPmYjlwIbm0NUEtlEr3Jw3g9v9tczcmZkXAO+nOms3wP8r+4IhR5oD9uhIAhjm9eGrq6l6RY506XvZ22YIbL7D7j5gXzlh31J3N8mgI2luPUw1jwfgfUzyqKDiKGcOB7WcVxr7uQhOR4DLZ2hZY7MQeF6hOlJK0hxx6EoSZc7L4TLPZkN9jssk7IHTk4Ypvw9SzX25bQ4D24r6OpT1GJvisNjl7h1Sd7NHR+p+K9rMQ5nK8E1rQvDwFMPSkSpTREZE/WzNm6cYnKYU2CJiRQlt9XVYN4UepRvLcpJqHpQnZZS6kGdGlubzA+hFPd2ukmaVQ1eSJMmgI0mSZNCRJEky6EiSJBl0JEmSDDqSJEkGHUmSZNCRJEky6EiaqrGI8Iy7PaRsz2OWhGTQkQQHgEGLoaesKdtVkkFH6ns/A95rMfSUS4BfWAxSZ/BaV9J8fgAjlgIvAVdl5guWSNdvz1XAfuDCzDxqiUjzzx4daR5l5nHgZmA4IhZYIl0dchYA3wU+b8iRDDqSXg87e6jmdLwYEWsska4MOa2enLHM/LYlInXQ59OhK6ljGssh4F5gN/AccDAzxyyZjt1eK6kmHl8CXEPVk2PIkQw6ks7SeA4AW4B3UB2NtcRS6VjHqHrifgE85HCVZNCRJEmaU87RkSRJBh1JkiSDjiRJkkFHkiTJoCNJkmTQkSRJMuhIkiSDjiRJkkFHkiTJoCNJkmTQkSRJmrb/D6SCNQI+LjJzAAAAAElFTkSuQmCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0.0313      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 31          |\n",
      "|    reward             | -0.26771596 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.9         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 13.1        |\n",
      "|    reward             | -0.79839075 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.77        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -92         |\n",
      "|    reward             | -0.68854123 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 10.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 40.5      |\n",
      "|    reward             | 1.0398989 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.81      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 478       |\n",
      "|    reward             | 3.0514724 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 146       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -0.295     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 145        |\n",
      "|    reward             | -2.0148928 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 14.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -31.1     |\n",
      "|    reward             | 1.0034695 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.82      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 91       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -204     |\n",
      "|    reward             | -2.34927 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 30.6     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 92          |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 48          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 180         |\n",
      "|    reward             | -0.35745117 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 21.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 92        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.129    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -85.1     |\n",
      "|    reward             | -2.623952 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.99      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -0.000708 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -168      |\n",
      "|    reward             | 0.7398023 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 29.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 93       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.5    |\n",
      "|    explained_variance | -0.0325  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -28.7    |\n",
      "|    reward             | 1.48759  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 4.16     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 53.8       |\n",
      "|    reward             | -3.7107315 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 74         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 110        |\n",
      "|    reward             | 0.34246665 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.55       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 27.9      |\n",
      "|    reward             | 17.672325 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.21      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 249       |\n",
      "|    reward             | 1.8335472 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 45.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 94       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -96.7    |\n",
      "|    reward             | 4.82384  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 62.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 3.97      |\n",
      "|    reward             | 0.7816919 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.0346    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -17.5      |\n",
      "|    reward             | -1.4671791 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.73       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 95        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -0.00333  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -109      |\n",
      "|    reward             | 0.7879932 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 95        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 330       |\n",
      "|    reward             | 3.2602186 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 69.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 95        |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | -99.7     |\n",
      "|    reward             | 2.4210072 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 95        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -0.000556 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -2.1e+03  |\n",
      "|    reward             | 29.021437 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.83e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 95         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 104        |\n",
      "|    reward             | 0.77756584 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.7       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 95       |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 48.2     |\n",
      "|    reward             | 2.895016 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.86     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 95        |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -31.5     |\n",
      "|    reward             | 1.4737622 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.29      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 95         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 141        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 74.8       |\n",
      "|    reward             | -0.6172008 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.93       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 95         |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 146        |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 624        |\n",
      "|    reward             | -0.6066415 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 335        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 95        |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -60.3     |\n",
      "|    reward             | -1.586635 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.75      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 95       |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 156      |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 18.4     |\n",
      "|    reward             | 0.729632 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.689    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 95         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 161        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | 71.1       |\n",
      "|    reward             | -0.1386866 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.52       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 166        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -438       |\n",
      "|    reward             | -2.4091787 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 114        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 171         |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | 55          |\n",
      "|    reward             | -0.61050427 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 3.84        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 176       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 408       |\n",
      "|    reward             | 2.2216203 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 125       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 181         |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | -0.0631     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 119         |\n",
      "|    reward             | 0.025970789 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 10.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 187       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -3.58e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -1.17     |\n",
      "|    reward             | 0.6213003 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 192       |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 211       |\n",
      "|    reward             | 1.1417162 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 31.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 3800        |\n",
      "|    time_elapsed       | 197         |\n",
      "|    total_timesteps    | 19000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3799        |\n",
      "|    policy_loss        | -16.9       |\n",
      "|    reward             | -0.68517995 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 5.34        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 202        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -0.0401    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 156        |\n",
      "|    reward             | -0.9455409 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 15.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 207       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -131      |\n",
      "|    reward             | 3.7680063 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 23.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 4100        |\n",
      "|    time_elapsed       | 212         |\n",
      "|    total_timesteps    | 20500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4099        |\n",
      "|    policy_loss        | -40.2       |\n",
      "|    reward             | -0.24572177 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.27        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 217         |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | -0.0508     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | -154        |\n",
      "|    reward             | -0.25915635 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 15.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 222        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -0.537     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 4.94       |\n",
      "|    reward             | 0.45847043 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.447      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 228        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | 119        |\n",
      "|    reward             | -1.9876399 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 15.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 233        |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 562        |\n",
      "|    reward             | -1.9555657 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 193        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 238       |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 105       |\n",
      "|    reward             | 5.17139   |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 10.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 243       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | 78.5      |\n",
      "|    reward             | 3.1070304 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 13.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 248        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0.000677   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -142       |\n",
      "|    reward             | -0.9928249 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 14.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 253       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | -0.567    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -105      |\n",
      "|    reward             | 2.0276108 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 8.8       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 258        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0.0828     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 149        |\n",
      "|    reward             | -3.0211833 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 13.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 263       |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 208       |\n",
      "|    reward             | 1.1533439 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 112       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 268       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -328      |\n",
      "|    reward             | 6.4672647 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 84.5      |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5275722.82\n",
      "total_reward: 4275722.82\n",
      "total_cost: 128413.35\n",
      "total_trades: 58102\n",
      "Sharpe: 0.846\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 5300        |\n",
      "|    time_elapsed       | 273         |\n",
      "|    total_timesteps    | 26500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | 0.0428      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5299        |\n",
      "|    policy_loss        | -7.54       |\n",
      "|    reward             | 0.009026352 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.16        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 279        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -0.0119    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | 85.8       |\n",
      "|    reward             | -2.1834304 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 5.56       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 284        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0.0176     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -27.5      |\n",
      "|    reward             | 0.22498888 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 14.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 289        |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | -162       |\n",
      "|    reward             | -0.5261431 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 14.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 294       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0.128     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 21.7      |\n",
      "|    reward             | 1.0538435 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.55      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 299        |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -0.411     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | 53.1       |\n",
      "|    reward             | -1.7570398 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 8.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 304        |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 41.9       |\n",
      "|    reward             | 0.39971724 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.46       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 309        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0.054      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 31         |\n",
      "|    reward             | 0.20150483 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.26       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 315       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 77.4      |\n",
      "|    reward             | 0.5911877 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.82      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 320        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -0.0324    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | 32.6       |\n",
      "|    reward             | 0.40631145 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 325        |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | 218        |\n",
      "|    reward             | -3.5734422 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 40.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 330        |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | -0.177     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | 15.8       |\n",
      "|    reward             | 0.35842443 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 335        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0.0173     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 116        |\n",
      "|    reward             | -2.8677428 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 15.1       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 96       |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 340      |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.1    |\n",
      "|    explained_variance | -0.0769  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | -54.3    |\n",
      "|    reward             | 0.798381 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 5.75     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 345       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0.00094   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -672      |\n",
      "|    reward             | -9.237123 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 431       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 350       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -175      |\n",
      "|    reward             | 1.7954694 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 17.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 355       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -0.00172  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -374      |\n",
      "|    reward             | 1.3194268 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 104       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 361       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -0.0355   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -39.2     |\n",
      "|    reward             | 0.8046928 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.64      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 366       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -0.22     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -43.5     |\n",
      "|    reward             | 2.2737834 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 5.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 371        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -0.584     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -24.3      |\n",
      "|    reward             | 0.67926747 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 376        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -99        |\n",
      "|    reward             | -2.0765526 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 7.59       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 96         |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 381        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | 721        |\n",
      "|    reward             | -17.651793 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 313        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 386       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 437       |\n",
      "|    reward             | -4.453337 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 118       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 96       |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 391      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | -37.3    |\n",
      "|    reward             | 2.11943  |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 1.73     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 397         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | -40.8       |\n",
      "|    reward             | -0.66978115 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 1.28        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 402         |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 2.38e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | -60.7       |\n",
      "|    reward             | -0.08960711 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 2.5         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 7900        |\n",
      "|    time_elapsed       | 407         |\n",
      "|    total_timesteps    | 39500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7899        |\n",
      "|    policy_loss        | 293         |\n",
      "|    reward             | -0.06681415 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 79.6        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 412       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0.0454    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -348      |\n",
      "|    reward             | 6.1656566 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 82.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 8100       |\n",
      "|    time_elapsed       | 417        |\n",
      "|    total_timesteps    | 40500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0.166      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8099       |\n",
      "|    policy_loss        | -127       |\n",
      "|    reward             | -7.9292135 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 134        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 97           |\n",
      "|    iterations         | 8200         |\n",
      "|    time_elapsed       | 422          |\n",
      "|    total_timesteps    | 41000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.8        |\n",
      "|    explained_variance | 0.0481       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8199         |\n",
      "|    policy_loss        | -33.3        |\n",
      "|    reward             | -0.040035784 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.866        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 427       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 112       |\n",
      "|    reward             | 1.9909316 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 8.88      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 432       |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0192    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | 8.08      |\n",
      "|    reward             | 1.7110631 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.886     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 437       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -37.4     |\n",
      "|    reward             | 3.3946385 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 443        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | -70.7      |\n",
      "|    reward             | -2.1325834 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 23.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 448       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.422     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -7.65     |\n",
      "|    reward             | 0.6848749 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 2.07      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 453       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | -0.123    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -34.4     |\n",
      "|    reward             | 0.9508995 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 2.07      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 458        |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0.222      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | -73.9      |\n",
      "|    reward             | 0.27870262 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 463        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | -0.00662   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | -36.3      |\n",
      "|    reward             | 0.61418766 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 6.93       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 9100        |\n",
      "|    time_elapsed       | 468         |\n",
      "|    total_timesteps    | 45500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9099        |\n",
      "|    policy_loss        | -104        |\n",
      "|    reward             | -0.37329662 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 25.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 473        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | -264       |\n",
      "|    reward             | -3.2187536 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 67.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 478       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -117      |\n",
      "|    reward             | 1.8304657 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 9.42      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 483        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 66.8       |\n",
      "|    reward             | 0.44312093 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 3.61       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 97       |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 488      |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | 135      |\n",
      "|    reward             | 0.856101 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 10.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 493       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | -0.00532  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | 165       |\n",
      "|    reward             | 0.6432389 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 17.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 498        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | -46.9      |\n",
      "|    reward             | -0.5203069 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 503       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | -54.8     |\n",
      "|    reward             | 6.8984942 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 45.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 9900        |\n",
      "|    time_elapsed       | 509         |\n",
      "|    total_timesteps    | 49500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9899        |\n",
      "|    policy_loss        | -12.1       |\n",
      "|    reward             | -0.33451855 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.499       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 514       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 50.8      |\n",
      "|    reward             | 0.6609639 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 2.31      |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
