{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwrAr8vEmP2M"
   },
   "source": [
    "# Introduction\n",
    "1. In this tutorial, we will be tuning hyperparameters for Stable baselines3 models using Optuna.\n",
    "2. The default model hyperparamters may not be adequate for your custom portfolio or custom state-space. Reinforcement learning algorithms are sensitive to hyperparamters, hence tuning is an important step.\n",
    "3. Hyperparamters are tuned based on an objective, which needs to be maximized or minimized. Here we tuned our hyperparamters to maximize the Sharpe Ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRF-7Vp5NCjU"
   },
   "outputs": [],
   "source": [
    "#Installing FinRL\n",
    "%%capture\n",
    "!pip install git+https://github.com/AI4Finance-LLC/FinRL-Library.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRJk36AZQGuh"
   },
   "outputs": [],
   "source": [
    "#Installing Optuna\n",
    "%%capture\n",
    "!pip3 install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqOKn-VWNGt4",
    "outputId": "32e0f056-37ad-4af4-f0de-71bcce6e5878"
   },
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "import optuna\n",
    "%matplotlib inline\n",
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "# Changed finrl_meta to meta\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv as StockTradingEnv_numpy\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.agents.rllib.models import DRLAgent as DRLAgent_rllib\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "import joblib\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "import ray\n",
    "from pprint import pprint\n",
    "\n",
    "# Not needed for local run\n",
    "# import sys\n",
    "# sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7_fCHS6NMx9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
    "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)  # datasets\n",
    "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
    "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)  # trained_models\n",
    "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
    "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)  # tensorboard_log\n",
    "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"./\" + config.RESULTS_DIR)  # results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71P6jMlEpikl"
   },
   "source": [
    "## Collecting data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frT5V9lLOv9X",
    "outputId": "9d05910d-aa9c-4273-ecd3-33e4098eabbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n",
      "Shape of DataFrame:  (3209, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (640, 8)\n"
     ]
    }
   ],
   "source": [
    "#Custom ticker list dataframe download\n",
    "# ticker_list = config_tickers.DOW_30_TICKER\n",
    "# df = YahooDownloader(start_date = '2009-01-01',\n",
    "#                      end_date = '2021-10-01',\n",
    "#                      ticker_list = ticker_list).fetch_data()\n",
    "\n",
    "df_list = []\n",
    "TRAIN_START_DATE = '2009-01-01'\n",
    "TEST_END_DATE = '2021-10-01'\n",
    "for ticker in config_tickers.DOW_30_TICKER:\n",
    "    # Fetch data for each ticker\n",
    "    portfolio_raw_df = YahooDownloader(start_date=TRAIN_START_DATE,\n",
    "                                       end_date=TEST_END_DATE,\n",
    "                                       ticker_list=[ticker]).fetch_data()\n",
    "    # Append the fetched DataFrame to the list\n",
    "    df_list.append(portfolio_raw_df)\n",
    "\n",
    "# Concatenate all DataFrames row-wise\n",
    "df = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date       open       high        low      close    volume  tic  day\n",
      "0  2009-01-02  15.014077  19.330000  19.520000  18.570000  10955700  AXP    4\n",
      "1  2009-01-05  15.495640  19.950001  20.240000  19.200001  16019200  AXP    0\n",
      "2  2009-01-06  16.365574  21.070000  21.379999  20.299999  13820200  AXP    1\n",
      "3  2009-01-07  15.676163  20.010000  20.719999  20.530001  15699900  AXP    2\n",
      "4  2009-01-08  15.699665  20.040001  20.170000  19.799999  12255100  AXP    3\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cntKg5nWO5qn",
    "outputId": "93a5b7cb-fc19-446b-da92-a43f070f6049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3208, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "#You can add technical indicators and turbulence factor to dataframe\n",
    "#Just set the use_technical_indicator=True, use_vix=True and use_turbulence=True\n",
    "fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = config.INDICATORS,\n",
    "                    use_vix=True,\n",
    "                    use_turbulence=True,\n",
    "                    user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | date       |    open |     high |      low |    close |    volume | tic   |   day |   macd |   boll_ub |   boll_lb |   rsi_30 |   cci_30 |   dx_30 |   close_30_sma |   close_60_sma |   vix |   turbulence |\n",
      "|---:|:-----------|--------:|---------:|---------:|---------:|----------:|:------|------:|-------:|----------:|----------:|---------:|---------:|--------:|---------------:|---------------:|------:|-------------:|\n",
      "|  0 | 2009-01-02 |  2.737  |  3.24107 |  3.25143 |  3.06714 | 746015200 | AAPL  |     4 |      0 |   3.56552 |   2.82912 |      100 |  66.6667 |     100 |        3.06714 |        3.06714 | 39.58 |            0 |\n",
      "|  1 | 2009-01-02 | 41.8162 | 58.99    | 59.08    | 58.59    |   6547900 | AMGN  |     4 |      0 |   3.56552 |   2.82912 |      100 |  66.6667 |     100 |       58.59    |       58.59    | 39.58 |            0 |\n",
      "|  2 | 2009-01-02 | 15.0141 | 19.33    | 19.52    | 18.57    |  10955700 | AXP   |     4 |      0 |   3.56552 |   2.82912 |      100 |  66.6667 |     100 |       18.57    |       18.57    | 39.58 |            0 |\n",
      "|  3 | 2009-01-02 | 33.9411 | 45.25    | 45.56    | 42.8     |   7010200 | BA    |     4 |      0 |   3.56552 |   2.82912 |      100 |  66.6667 |     100 |       42.8     |       42.8     | 39.58 |            0 |\n",
      "|  4 | 2009-01-02 | 30.6026 | 46.91    | 46.98    | 44.91    |   7117200 | CAT   |     4 |      0 |   3.56552 |   2.82912 |      100 |  66.6667 |     100 |       44.91    |       44.91    | 39.58 |            0 |\n"
     ]
    }
   ],
   "source": [
    "print(processed.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5diXih4zPE6m"
   },
   "outputs": [],
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)\n",
    "processed_full.sort_values(['date','tic'],ignore_index=True).head(5)\n",
    "\n",
    "processed_full.to_csv('processed_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RrJiFbSPKE2",
    "outputId": "5ca01b3e-8e7d-4e78-8d3f-289b8f3f8674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83897\n",
      "10353\n"
     ]
    }
   ],
   "source": [
    "train = data_split(processed_full, '2009-01-01','2020-07-01')\n",
    "trade = data_split(processed_full, '2020-05-01','2021-10-01')\n",
    "print(len(train))\n",
    "print(len(trade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ub4JTTRcPOel",
    "outputId": "432716f9-82ed-463b-c4ac-a0901fe5a0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(config.INDICATORS) * stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YiF95zXgPTsd"
   },
   "outputs": [],
   "source": [
    "#Defining the environment kwargs\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "# From: https://github.com/AI4Finance-Foundation/FinRL/issues/540\\\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,  # added argument\n",
    "    \"buy_cost_pct\": buy_cost_list,  # changed to list\n",
    "    \"sell_cost_pct\": sell_cost_list,  # changed to list\n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": config.INDICATORS, \n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4\n",
    "    \n",
    "}\n",
    "#Instantiate the training gym compatible environment\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "892NcZALPWHF",
    "outputId": "88d8958a-a9aa-4fbf-9338-c990f7829a1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the training environment\n",
    "# Also instantiate our training gent\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))\n",
    "agent = DRLAgent(env = env_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3EwB1T8opX2o"
   },
   "outputs": [],
   "source": [
    "#Instantiate the trading environment\n",
    "e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = None, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOynTQluppye"
   },
   "source": [
    "## Tuning hyperparameters using Optuna\n",
    "1. Go to this [link](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py), you will find all possible hyperparamters to tune for all the models.\n",
    "2. For your model, grab those hyperparamters which you want to optimize and then return a dictionary of hyperparamters.\n",
    "3. There is a feature in Optuna called as hyperparamters importance, you can point out those hyperparamters which are important for tuning.\n",
    "4. By default Optuna use [TPESampler](https://www.youtube.com/watch?v=tdwgR1AqQ8Y) for sampling hyperparamters from the search space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_vojRvAsP2ja"
   },
   "outputs": [],
   "source": [
    "def sample_ddpg_params(trial:optuna.Trial):\n",
    "  # Size of the replay buffer\n",
    "  buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(1e5), int(1e6)])\n",
    "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "  batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256, 512])\n",
    "  \n",
    "  return {\"buffer_size\": buffer_size,\n",
    "          \"learning_rate\":learning_rate,\n",
    "          \"batch_size\":batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_a2c_params(trial:optuna.Trial):\n",
    "    # learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1, log=True)  # fix deprecation\n",
    "    n_steps = trial.suggest_categorical(\n",
    "        \"n_steps\", [8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "    )\n",
    "    # ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)  # fix deprecation\n",
    "    # vf_coef = trial.suggest_uniform(\"vf_coef\", 0, 1)\n",
    "    return {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"ent_coef\": ent_coef\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "xL7LeLeWrj6H"
   },
   "outputs": [],
   "source": [
    "#Calculate the Sharpe ratio\n",
    "#This is our objective for tuning\n",
    "def calculate_sharpe(df):\n",
    "    df['daily_return'] = df['account_value'].pct_change(1)\n",
    "    if df['daily_return'].std() != 0:\n",
    "        sharpe = (252 ** 0.5) * df['daily_return'].mean()/ \\\n",
    "            df['daily_return'].std()\n",
    "        return sharpe\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCRy_kL648DM"
   },
   "source": [
    "## Callbacks\n",
    "1. The callback will terminate if the improvement margin is below certain point\n",
    "2. It will terminate after certain number of trial_number are reached, not before that\n",
    "3. It will hold its patience to reach the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingCallback:\n",
    "    def __init__(self, threshold: int, trial_number: int, patience: int):\n",
    "        \"\"\"\n",
    "        threshold:int tolerance for increase in sharpe ratio\n",
    "        trial_number: int Prune after minimum number of trials\n",
    "        patience: int patience for the threshold\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.trial_number = trial_number\n",
    "        self.patience = patience\n",
    "        self.cb_list = []  # Trials list for which threshold is reached\n",
    "\n",
    "    def __call__(self, study: optuna.study, frozen_trial: optuna.Trial):\n",
    "        # Setting the best value in the current trial\n",
    "        study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "\n",
    "        # Checking if the minimum number of trials have pass\n",
    "        if frozen_trial.number > self.trial_number:\n",
    "            previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
    "            # Checking if the previous and current objective values have the same sign\n",
    "            if previous_best_value * study.best_value >= 0:\n",
    "                # Checking for the threshold condition\n",
    "                if abs(previous_best_value - study.best_value) < self.threshold:\n",
    "                    self.cb_list.append(frozen_trial.number)\n",
    "                    # If threshold is achieved for the patience amount of time\n",
    "                    if len(self.cb_list) > self.patience:\n",
    "                        print(\"The study stops now...\")\n",
    "                        print(\n",
    "                            \"With number\",\n",
    "                            frozen_trial.number,\n",
    "                            \"and value \",\n",
    "                            frozen_trial.value,\n",
    "                        )\n",
    "                        print(\n",
    "                            \"The previous and current best values are {} and {} respectively\".format(\n",
    "                                previous_best_value, study.best_value\n",
    "                            )\n",
    "                        )\n",
    "                        study.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import sys   \n",
    "\n",
    "os.makedirs(\"A2C_optuna_models\",exist_ok=True)\n",
    "\n",
    "def objective(trial: optuna.Trial):  # Optuna objective\n",
    "    hyperparameters = sample_a2c_params(trial)\n",
    "    model_a2c = agent.get_model(\"a2c\", model_kwargs=hyperparameters)  # TODO: Could try out self-implemented A2C\n",
    "    trained_model = agent.train_model(model=model_a2c, tb_log_name=\"a2c\", total_timesteps=10000)  # train stablebaselines3's A2C\n",
    "    trained_model.save('A2C_optuna_models/a2c_{}.pth'.format(trial.number))  # save model with trial number as ID\n",
    "    # clear_output(wait=True)  # This will keep only the last trial tested\n",
    "    df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
    "        model=trained_model,\n",
    "        environment=e_trade_gym\n",
    "    )\n",
    "    sharpe = calculate_sharpe(df_account_value)\n",
    "\n",
    "    return sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:08:38,145] A new study created in memory with name: a2c_study\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 9.069790423538583e-05, 'n_steps': 1024, 'ent_coef': 3.1968399196034683e-06}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:10:09,130] Trial 0 finished with value: 1.0467206287624573 and parameters: {'learning_rate': 9.069790423538583e-05, 'n_steps': 1024, 'ent_coef': 3.1968399196034683e-06}. Best is trial 0 with value: 1.0467206287624573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.0031987155943821507, 'n_steps': 1024, 'ent_coef': 0.00020312961670857716}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2024-11-06 14:11:41,778] Trial 1 finished with value: 1.2321234462758197 and parameters: {'learning_rate': 0.0031987155943821507, 'n_steps': 1024, 'ent_coef': 0.00020312961670857716}. Best is trial 1 with value: 1.2321234462758197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 2.38180503070213e-05, 'n_steps': 16, 'ent_coef': 0.004121293029638071}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 1600      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.173     |\n",
      "|    learning_rate      | 2.38e-05  |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 83.4      |\n",
      "|    reward             | 1.2544177 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.32      |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3551227.09\n",
      "total_reward: 2551227.09\n",
      "total_cost: 401478.64\n",
      "total_trades: 80949\n",
      "Sharpe: 0.776\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 101         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 3200        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0.0199      |\n",
      "|    learning_rate      | 2.38e-05    |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 87.5        |\n",
      "|    reward             | -0.11346094 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 6.37        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 4800      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.206    |\n",
      "|    learning_rate      | 2.38e-05  |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 129       |\n",
      "|    reward             | -0.408095 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 19.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 6400       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.0473     |\n",
      "|    learning_rate      | 2.38e-05   |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -97.7      |\n",
      "|    reward             | 0.21759926 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 7.26       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 2.38e-05  |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -8.93     |\n",
      "|    reward             | 1.7986554 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.55      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 9600       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.0205     |\n",
      "|    learning_rate      | 2.38e-05   |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 48.4       |\n",
      "|    reward             | -0.5039934 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.16       |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:13:18,323] Trial 2 finished with value: 0.40917030757284817 and parameters: {'learning_rate': 2.38180503070213e-05, 'n_steps': 16, 'ent_coef': 0.004121293029638071}. Best is trial 1 with value: 1.2321234462758197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 5.2340048721427795e-05, 'n_steps': 64, 'ent_coef': 0.0005298666736139674}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 112        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 6400       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.153      |\n",
      "|    learning_rate      | 5.23e-05   |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 48.2       |\n",
      "|    reward             | 0.21873954 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.97       |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:14:48,217] Trial 3 finished with value: 0.528719921300583 and parameters: {'learning_rate': 5.2340048721427795e-05, 'n_steps': 64, 'ent_coef': 0.0005298666736139674}. Best is trial 1 with value: 1.2321234462758197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.009399914420398066, 'n_steps': 512, 'ent_coef': 0.00020993843801086698}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4911662.03\n",
      "total_reward: 3911662.03\n",
      "total_cost: 10391.64\n",
      "total_trades: 41792\n",
      "Sharpe: 0.835\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:16:19,541] Trial 4 finished with value: 1.5110648087592045 and parameters: {'learning_rate': 0.009399914420398066, 'n_steps': 512, 'ent_coef': 0.00020993843801086698}. Best is trial 4 with value: 1.5110648087592045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.36359209473364146, 'n_steps': 16, 'ent_coef': 2.3829547523593072e-08}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 110       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 1600      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -32.4     |\n",
      "|    explained_variance | -1.67e-06 |\n",
      "|    learning_rate      | 0.364     |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 77.7      |\n",
      "|    reward             | 1.1691    |\n",
      "|    std                | 235       |\n",
      "|    value_loss         | 8.41      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 111         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 3200        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.364       |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 130         |\n",
      "|    reward             | -0.25453633 |\n",
      "|    std                | 388         |\n",
      "|    value_loss         | 13.3        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 111         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 43          |\n",
      "|    total_timesteps    | 4800        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | -1.55e-05   |\n",
      "|    learning_rate      | 0.364       |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | 38.2        |\n",
      "|    reward             | -0.37080386 |\n",
      "|    std                | 591         |\n",
      "|    value_loss         | 14.5        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 110         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 57          |\n",
      "|    total_timesteps    | 6400        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -28.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.364       |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -86.6       |\n",
      "|    reward             | -0.19986372 |\n",
      "|    std                | 1.11e+03    |\n",
      "|    value_loss         | 11.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 110       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -25.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.364     |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 126       |\n",
      "|    reward             | 1.7903554 |\n",
      "|    std                | 751       |\n",
      "|    value_loss         | 54.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 110         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 86          |\n",
      "|    total_timesteps    | 9600        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -22.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.364       |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -6.1        |\n",
      "|    reward             | -0.08358457 |\n",
      "|    std                | 1.22e+03    |\n",
      "|    value_loss         | 2.17        |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:17:51,089] Trial 5 finished with value: 1.3695975223072174 and parameters: {'learning_rate': 0.36359209473364146, 'n_steps': 16, 'ent_coef': 2.3829547523593072e-08}. Best is trial 4 with value: 1.5110648087592045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.001812349871305581, 'n_steps': 8, 'ent_coef': 8.265607234648155e-06}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 112        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 800        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.0311    |\n",
      "|    learning_rate      | 0.00181    |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 234        |\n",
      "|    reward             | 0.57526433 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 34.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 113       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 1600      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.00181   |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 224       |\n",
      "|    reward             | 4.2116585 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 47.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 118        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 2400       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.00181    |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 73.9       |\n",
      "|    reward             | -3.5629795 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.45       |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 116           |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 27            |\n",
      "|    total_timesteps    | 3200          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -41.2         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.00181       |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | 142           |\n",
      "|    reward             | -0.0061449385 |\n",
      "|    std                | 1             |\n",
      "|    value_loss         | 13.3          |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 115        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.00181    |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 89.1       |\n",
      "|    reward             | -2.4857092 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.54       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 114       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 4800      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.00181   |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 202       |\n",
      "|    reward             | 0.8730149 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 29.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 114      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 5600     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.00181  |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 275      |\n",
      "|    reward             | 8.942954 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 58.4     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 114          |\n",
      "|    iterations         | 800          |\n",
      "|    time_elapsed       | 55           |\n",
      "|    total_timesteps    | 6400         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.1        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.00181      |\n",
      "|    n_updates          | 799          |\n",
      "|    policy_loss        | 7.85         |\n",
      "|    reward             | -0.062054813 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 1.59         |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 114      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 7200     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.00181  |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -83.4    |\n",
      "|    reward             | 9.002824 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 9.45     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 114       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.00181   |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 18.1      |\n",
      "|    reward             | 7.4695573 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 2.76      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 114       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 8800      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -40.9     |\n",
      "|    explained_variance | -0.000388 |\n",
      "|    learning_rate      | 0.00181   |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -138      |\n",
      "|    reward             | 1.7498525 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 13.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 114         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 84          |\n",
      "|    total_timesteps    | 9600        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -40.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.00181     |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 0.66        |\n",
      "|    reward             | -0.59496135 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 0.391       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:19:19,628] Trial 6 finished with value: 1.3627999357161382 and parameters: {'learning_rate': 0.001812349871305581, 'n_steps': 8, 'ent_coef': 8.265607234648155e-06}. Best is trial 4 with value: 1.5110648087592045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 356, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1364529.71\n",
      "total_reward: 364529.71\n",
      "total_cost: 1975.88\n",
      "total_trades: 6981\n",
      "Sharpe: 1.363\n",
      "=================================\n",
      "hit end!\n",
      "{'learning_rate': 0.004778133862873408, 'n_steps': 128, 'ent_coef': 0.006562192211340152}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2346458.10\n",
      "total_reward: 1346458.10\n",
      "total_cost: 54797.22\n",
      "total_trades: 50391\n",
      "Sharpe: 0.544\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:20:42,372] Trial 7 finished with value: 1.2668300270488502 and parameters: {'learning_rate': 0.004778133862873408, 'n_steps': 128, 'ent_coef': 0.006562192211340152}. Best is trial 4 with value: 1.5110648087592045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.014744459088243693, 'n_steps': 128, 'ent_coef': 2.510784660114704e-08}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2024-11-06 14:22:03,108] Trial 8 finished with value: 0.9096160921768621 and parameters: {'learning_rate': 0.014744459088243693, 'n_steps': 128, 'ent_coef': 2.510784660114704e-08}. Best is trial 4 with value: 1.5110648087592045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.022238665134707736, 'n_steps': 128, 'ent_coef': 0.04525524814651032}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3208457.03\n",
      "total_reward: 2208457.03\n",
      "total_cost: 3160.22\n",
      "total_trades: 54306\n",
      "Sharpe: 0.710\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:23:28,614] Trial 9 finished with value: 1.7751415667028998 and parameters: {'learning_rate': 0.022238665134707736, 'n_steps': 128, 'ent_coef': 0.04525524814651032}. Best is trial 9 with value: 1.7751415667028998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.4579045629871961, 'n_steps': 32, 'ent_coef': 0.09498013326989069}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 3200       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -98.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.458      |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -1.17e+03  |\n",
      "|    reward             | 0.48748147 |\n",
      "|    std                | 1.48e+03   |\n",
      "|    value_loss         | 159        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 6400      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -144      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.458     |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 728       |\n",
      "|    reward             | 0.5252056 |\n",
      "|    std                | 1.53e+06  |\n",
      "|    value_loss         | 31.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 126        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 9600       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -155       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.458      |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -314       |\n",
      "|    reward             | -0.7407671 |\n",
      "|    std                | 1.01e+05   |\n",
      "|    value_loss         | 10.6       |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:24:50,001] Trial 10 finished with value: 1.5154913501978873 and parameters: {'learning_rate': 0.4579045629871961, 'n_steps': 32, 'ent_coef': 0.09498013326989069}. Best is trial 9 with value: 1.7751415667028998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.6911073279866081, 'n_steps': 32, 'ent_coef': 0.07807076488793632}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 116        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 3200       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -103       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.691      |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -921       |\n",
      "|    reward             | -0.6632277 |\n",
      "|    std                | 824        |\n",
      "|    value_loss         | 97.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 117         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 54          |\n",
      "|    total_timesteps    | 6400        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -120        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.691       |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 81.7        |\n",
      "|    reward             | -0.14142121 |\n",
      "|    std                | 5.99e+03    |\n",
      "|    value_loss         | 2.25        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 114         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 83          |\n",
      "|    total_timesteps    | 9600        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -151        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.691       |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | 1.18e+03    |\n",
      "|    reward             | -0.64073396 |\n",
      "|    std                | 6.02e+05    |\n",
      "|    value_loss         | 91.3        |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:26:18,524] Trial 11 finished with value: 1.4973116170530973 and parameters: {'learning_rate': 0.6911073279866081, 'n_steps': 32, 'ent_coef': 0.07807076488793632}. Best is trial 9 with value: 1.7751415667028998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.10276422167929608, 'n_steps': 32, 'ent_coef': 0.06461742577600674}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4393603.03\n",
      "total_reward: 3393603.03\n",
      "total_cost: 23487.95\n",
      "total_trades: 49804\n",
      "Sharpe: 0.804\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 107         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 3200        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.103       |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -441        |\n",
      "|    reward             | -0.38010812 |\n",
      "|    std                | 14.3        |\n",
      "|    value_loss         | 99.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 6400       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -67.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.103      |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 355        |\n",
      "|    reward             | 0.11730957 |\n",
      "|    std                | 62.6       |\n",
      "|    value_loss         | 39.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 111        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 85         |\n",
      "|    total_timesteps    | 9600       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -90.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.103      |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 480        |\n",
      "|    reward             | -0.3485061 |\n",
      "|    std                | 327        |\n",
      "|    value_loss         | 39.7       |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:27:48,825] Trial 12 finished with value: 1.4927361570680404 and parameters: {'learning_rate': 0.10276422167929608, 'n_steps': 32, 'ent_coef': 0.06461742577600674}. Best is trial 9 with value: 1.7751415667028998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.037460050210520165, 'n_steps': 256, 'ent_coef': 0.006663008686116493}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2024-11-06 14:29:16,274] Trial 13 finished with value: 1.6431194022832425 and parameters: {'learning_rate': 0.037460050210520165, 'n_steps': 256, 'ent_coef': 0.006663008686116493}. Best is trial 9 with value: 1.7751415667028998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.04131516139407045, 'n_steps': 256, 'ent_coef': 0.005879003863387715}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3496281.81\n",
      "total_reward: 2496281.81\n",
      "total_cost: 2337.04\n",
      "total_trades: 38845\n",
      "Sharpe: 0.728\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:30:44,742] Trial 14 finished with value: 1.434404789325061 and parameters: {'learning_rate': 0.04131516139407045, 'n_steps': 256, 'ent_coef': 0.005879003863387715}. Best is trial 9 with value: 1.7751415667028998.\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.0005721124989524309, 'n_steps': 256, 'ent_coef': 0.0011505406271831633}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:32:12,339] Trial 15 finished with value: 1.3206598711777402 and parameters: {'learning_rate': 0.0005721124989524309, 'n_steps': 256, 'ent_coef': 0.0011505406271831633}. Best is trial 9 with value: 1.7751415667028998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.0720661604820144, 'n_steps': 2048, 'ent_coef': 4.009097084190781e-05}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2024-11-06 14:33:38,234] Trial 16 finished with value: 1.7893212360303676 and parameters: {'learning_rate': 0.0720661604820144, 'n_steps': 2048, 'ent_coef': 4.009097084190781e-05}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 356, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1540799.98\n",
      "total_reward: 540799.98\n",
      "total_cost: 998.99\n",
      "total_trades: 4272\n",
      "Sharpe: 1.789\n",
      "=================================\n",
      "hit end!\n",
      "{'learning_rate': 0.11077319352350509, 'n_steps': 2048, 'ent_coef': 2.9511016700070576e-07}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3183461.96\n",
      "total_reward: 2183461.96\n",
      "total_cost: 226101.83\n",
      "total_trades: 69714\n",
      "Sharpe: 0.652\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:35:09,650] Trial 17 finished with value: 1.3579212893575994 and parameters: {'learning_rate': 0.11077319352350509, 'n_steps': 2048, 'ent_coef': 2.9511016700070576e-07}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.1313631722525307, 'n_steps': 2048, 'ent_coef': 3.515179870104018e-05}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2024-11-06 14:36:39,721] Trial 18 finished with value: 1.459772772250219 and parameters: {'learning_rate': 0.1313631722525307, 'n_steps': 2048, 'ent_coef': 3.515179870104018e-05}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.0007292444504090859, 'n_steps': 128, 'ent_coef': 4.978111945260601e-07}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5900295.27\n",
      "total_reward: 4900295.27\n",
      "total_cost: 216671.07\n",
      "total_trades: 65757\n",
      "Sharpe: 0.943\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:38:08,097] Trial 19 finished with value: 1.4578761421597068 and parameters: {'learning_rate': 0.0007292444504090859, 'n_steps': 128, 'ent_coef': 4.978111945260601e-07}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.020228108874944747, 'n_steps': 2048, 'ent_coef': 1.7087499197772204e-05}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2024-11-06 14:39:42,577] Trial 20 finished with value: 1.5274815175513323 and parameters: {'learning_rate': 0.020228108874944747, 'n_steps': 2048, 'ent_coef': 1.7087499197772204e-05}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.03354432122000949, 'n_steps': 256, 'ent_coef': 0.015038797027987558}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2024-11-06 14:41:10,529] Trial 21 finished with value: 1.486846644618596 and parameters: {'learning_rate': 0.03354432122000949, 'n_steps': 256, 'ent_coef': 0.015038797027987558}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.0760536710731251, 'n_steps': 512, 'ent_coef': 0.018450789051394325}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3413081.30\n",
      "total_reward: 2413081.30\n",
      "total_cost: 42965.41\n",
      "total_trades: 47559\n",
      "Sharpe: 0.819\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:42:37,190] Trial 22 finished with value: 1.6887648326819964 and parameters: {'learning_rate': 0.0760536710731251, 'n_steps': 512, 'ent_coef': 0.018450789051394325}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.20349866496274885, 'n_steps': 512, 'ent_coef': 7.24921025913154e-05}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2024-11-06 14:44:04,109] Trial 23 finished with value: 1.6260066771547579 and parameters: {'learning_rate': 0.20349866496274885, 'n_steps': 512, 'ent_coef': 7.24921025913154e-05}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.06812796382784457, 'n_steps': 512, 'ent_coef': 0.001266671862863341}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5333110.75\n",
      "total_reward: 4333110.75\n",
      "total_cost: 2806.67\n",
      "total_trades: 33536\n",
      "Sharpe: 0.943\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:45:33,214] Trial 24 finished with value: 1.3020355140487767 and parameters: {'learning_rate': 0.06812796382784457, 'n_steps': 512, 'ent_coef': 0.001266671862863341}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.008592159025154643, 'n_steps': 8, 'ent_coef': 0.018743417515056645}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 800       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.00859   |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 279       |\n",
      "|    reward             | 1.4657485 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 39.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 96        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 1600      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.00859   |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 107       |\n",
      "|    reward             | 3.2858884 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 9.6       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 99        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 2400      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | -0.00117  |\n",
      "|    learning_rate      | 0.00859   |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 171       |\n",
      "|    reward             | -4.645365 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 31.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 101        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 3200       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.00859    |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 21.2       |\n",
      "|    reward             | 0.29869524 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.368      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 102        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.00859    |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 26.6       |\n",
      "|    reward             | -3.4456172 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.493      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 4800      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.00859   |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 189       |\n",
      "|    reward             | 0.8537669 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 21.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 103      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 5600     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.00859  |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 56.5     |\n",
      "|    reward             | 5.900373 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 7.89     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 104          |\n",
      "|    iterations         | 800          |\n",
      "|    time_elapsed       | 61           |\n",
      "|    total_timesteps    | 6400         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -43.5        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.00859      |\n",
      "|    n_updates          | 799          |\n",
      "|    policy_loss        | 53.2         |\n",
      "|    reward             | -0.019782105 |\n",
      "|    std                | 1.13         |\n",
      "|    value_loss         | 2.89         |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 104      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 7200     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.00859  |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -171     |\n",
      "|    reward             | 2.188979 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 19.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 104      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 76       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.00859  |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 135      |\n",
      "|    reward             | 6.386121 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 13.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 8800      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.00859   |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -48.5     |\n",
      "|    reward             | 1.4768764 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 91          |\n",
      "|    total_timesteps    | 9600        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.00859     |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -7.41       |\n",
      "|    reward             | -0.82239485 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 0.287       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:47:09,499] Trial 25 finished with value: 1.5583527224375198 and parameters: {'learning_rate': 0.008592159025154643, 'n_steps': 8, 'ent_coef': 0.018743417515056645}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.1878881903723904, 'n_steps': 64, 'ent_coef': 2.044250520221642e-06}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 114      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 6400     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -35.3    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.188    |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -231     |\n",
      "|    reward             | 0.339887 |\n",
      "|    std                | 11.8     |\n",
      "|    value_loss         | 47.7     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:48:38,143] Trial 26 finished with value: 1.707410524917026 and parameters: {'learning_rate': 0.1878881903723904, 'n_steps': 64, 'ent_coef': 2.044250520221642e-06}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 356, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1550754.30\n",
      "total_reward: 550754.30\n",
      "total_cost: 998.97\n",
      "total_trades: 5340\n",
      "Sharpe: 1.707\n",
      "=================================\n",
      "hit end!\n",
      "{'learning_rate': 0.21428442513566318, 'n_steps': 64, 'ent_coef': 8.906276757349905e-07}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2702145.27\n",
      "total_reward: 1702145.27\n",
      "total_cost: 28612.67\n",
      "total_trades: 42276\n",
      "Sharpe: 0.592\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 112       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 6400      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -27.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.214     |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 16        |\n",
      "|    reward             | 1.1665299 |\n",
      "|    std                | 14.4      |\n",
      "|    value_loss         | 5.83      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:50:08,157] Trial 27 finished with value: 1.5002806008378098 and parameters: {'learning_rate': 0.21428442513566318, 'n_steps': 64, 'ent_coef': 8.906276757349905e-07}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.8926488060017259, 'n_steps': 64, 'ent_coef': 9.047331185197167e-08}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 6400       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | 18         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.893      |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 75.3       |\n",
      "|    reward             | 0.26836628 |\n",
      "|    std                | 2.44e+03   |\n",
      "|    value_loss         | 169        |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:51:42,687] Trial 28 finished with value: 1.3383390191981641 and parameters: {'learning_rate': 0.8926488060017259, 'n_steps': 64, 'ent_coef': 9.047331185197167e-08}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "{'learning_rate': 0.001465603914067428, 'n_steps': 1024, 'ent_coef': 2.1776324009492536e-06}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_53992\\1594191206.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3125653.92\n",
      "total_reward: 2125653.92\n",
      "total_cost: 211792.87\n",
      "total_trades: 66268\n",
      "Sharpe: 0.612\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 14:53:11,570] Trial 29 finished with value: 1.525426542844777 and parameters: {'learning_rate': 0.001465603914067428, 'n_steps': 1024, 'ent_coef': 2.1776324009492536e-06}. Best is trial 16 with value: 1.7893212360303676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "# Create a study object and specify the direction as 'maximize'\n",
    "# As you want to maximize sharpe\n",
    "# Pruner stops not promising iterations\n",
    "# Use a pruner, else you will get error related to divergence of model\n",
    "# You can also use Multivariate samplere\n",
    "# sampler = optuna.samplers.TPESampler(multivarite=True,seed=42)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=1234)\n",
    "study = optuna.create_study(\n",
    "    study_name='a2c_study',\n",
    "    direction='maximize',\n",
    "    sampler=sampler,\n",
    "    pruner=optuna.pruners.HyperbandPruner(),\n",
    ")\n",
    "logging_callback = LoggingCallback(threshold=1e-5, trial_number=5, patience=30)\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=30,\n",
    "    catch=(ValueError,),\n",
    "    callbacks=[logging_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_a2c_study__.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(study, \"final_a2c_study__.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import sys   \n",
    "\n",
    "os.makedirs(\"DDPG_optuna_models\",exist_ok=True)\n",
    "\n",
    "def objective(trial:optuna.Trial):\n",
    "  #Trial will suggest a set of hyperparamters from the specified range\n",
    "  hyperparameters = sample_ddpg_params(trial)\n",
    "  model_ddpg = agent.get_model(\"ddpg\",model_kwargs = hyperparameters )\n",
    "  #You can increase it for better comparison\n",
    "  trained_ddpg = agent.train_model(model=model_ddpg,\n",
    "                                  tb_log_name=\"ddpg\" ,\n",
    "                             total_timesteps=50000)\n",
    "  trained_ddpg.save('models/ddpg_{}.pth'.format(trial.number))\n",
    "  clear_output(wait=True)\n",
    "  #For the given hyperparamters, determine the account value in the trading period\n",
    "  df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
    "    model=trained_ddpg, \n",
    "    environment = e_trade_gym)\n",
    "  #Calculate sharpe from the account value\n",
    "  sharpe = calculate_sharpe(df_account_value)\n",
    "\n",
    "  return sharpe\n",
    "\n",
    "#Create a study object and specify the direction as 'maximize'\n",
    "#As you want to maximize sharpe\n",
    "#Pruner stops not promising iterations\n",
    "#Use a pruner, else you will get error related to divergence of model\n",
    "#You can also use Multivariate samplere\n",
    "#sampler = optuna.samplers.TPESampler(multivarite=True,seed=42)\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"ddpg_study\",direction='maximize',\n",
    "                            sampler = sampler, pruner=optuna.pruners.HyperbandPruner())\n",
    "\n",
    "logging_callback = LoggingCallback(threshold=1e-5,patience=30,trial_number=5)\n",
    "#You can increase the n_trials for a better search space scanning\n",
    "study.optimize(objective, n_trials=30,catch=(ValueError,),callbacks=[logging_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sg7LRlHmj9GB",
    "outputId": "b089f7d6-7e92-44f1-b535-5ed127105366"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_ddpg_study__.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(study, \"final_ddpg_study__.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVMue1-xuGHC",
    "outputId": "115d9f90-081f-4ef0-e687-2d06eea505fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters after tuning {'learning_rate': 0.0720661604820144, 'n_steps': 2048, 'ent_coef': 4.009097084190781e-05}\n",
      "Hyperparameters before tuning {'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n"
     ]
    }
   ],
   "source": [
    "#Get the best hyperparamters\n",
    "print('Hyperparameters after tuning',study.best_params)\n",
    "print('Hyperparameters before tuning',config.A2C_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsZmMw0ykmYo",
    "outputId": "0da397f4-b412-49f5-aa93-34cdc4bcbf8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=16, state=TrialState.COMPLETE, values=[1.7893212360303676], datetime_start=datetime.datetime(2024, 11, 6, 14, 32, 12, 340339), datetime_complete=datetime.datetime(2024, 11, 6, 14, 33, 38, 233786), params={'learning_rate': 0.0720661604820144, 'n_steps': 2048, 'ent_coef': 4.009097084190781e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=1.0, log=True, low=1e-05, step=None), 'n_steps': CategoricalDistribution(choices=(8, 16, 32, 64, 128, 256, 512, 1024, 2048)), 'ent_coef': FloatDistribution(high=0.1, log=True, low=1e-08, step=None)}, trial_id=16, value=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "fETqKJj4uSi5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from stable_baselines3 import DDPG\n",
    "# tuned_model_ddpg = DDPG.load('models/ddpg_{}.pth'.format(study.best_trial.number),env=env_train)\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "tuned_model_a2c = A2C.load('A2C_optuna_models/a2c_{}.pth'.format(study.best_trial.number), env=env_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgchX1LLuua-",
    "outputId": "00b50778-ae4a-49f8-c676-2d5286d899c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "#Trading period account value with tuned model\n",
    "df_account_value_tuned, df_actions_tuned = DRLAgent.DRL_prediction(\n",
    "    model=tuned_model_a2c, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  account_value\n",
      "0  2020-05-01   1.000000e+06\n",
      "1  2020-05-04   9.978485e+05\n",
      "2  2020-05-05   1.001751e+06\n",
      "3  2020-05-06   1.003632e+06\n",
      "4  2020-05-07   1.000501e+06\n",
      "            AAPL  AMGN  AXP  BA  CAT  CRM  ...  TRV  UNH  V   VZ  WBA  WMT\n",
      "date                                       ...                            \n",
      "2020-05-01   100     0  100   0    0  100  ...    0  100  0  100    0  100\n",
      "2020-05-04   100     0  100   0    0  100  ...    0  100  0  100    0  100\n",
      "2020-05-05   100     0  100   0    0  100  ...    0  100  0  100    0  100\n",
      "2020-05-06   100     0  100   0    0  100  ...    0  100  0  100    0  100\n",
      "2020-05-07   100     0  100   0    0  100  ...    0  100  0  100    0  100\n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_account_value_tuned.head())\n",
    "print(df_actions_tuned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8s5KNvVuvr2D",
    "outputId": "6a4e6933-84ca-4689-b8c3-042fc77e181c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return          0.356834\n",
      "Cumulative returns     0.540800\n",
      "Annual volatility      0.180128\n",
      "Sharpe ratio           1.789321\n",
      "Calmar ratio           3.765003\n",
      "Stability              0.965409\n",
      "Max drawdown          -0.094777\n",
      "Omega ratio            1.360814\n",
      "Sortino ratio          2.816772\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.023460\n",
      "Daily value at risk   -0.021415\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Backtesting with our pruned model\n",
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all_tuned = backtest_stats(account_value=df_account_value_tuned)\n",
    "perf_stats_all_tuned = pd.DataFrame(perf_stats_all_tuned)\n",
    "perf_stats_all_tuned.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_tuned_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuGaI9lSvvVD",
    "outputId": "7e72920e-84e5-486f-e7ce-747d282783d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\frank\\.conda\\envs\\m_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0485   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -1.4      |\n",
      "|    reward             | 0.5177753 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.308     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 99        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0188   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -37.4     |\n",
      "|    reward             | 1.1794674 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.79      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 15.6      |\n",
      "|    reward             | -2.438506 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.02      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 274        |\n",
      "|    reward             | -1.5679361 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 74.5       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 103      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.86e+03 |\n",
      "|    reward             | 9.439849 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 2.11e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 102        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.0378     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 130        |\n",
      "|    reward             | -1.5823889 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 102        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.0704     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -81        |\n",
      "|    reward             | -1.0517981 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 11.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 102        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 105        |\n",
      "|    reward             | -1.2161883 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 123       |\n",
      "|    reward             | 1.4174336 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 10.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 102        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -91.4      |\n",
      "|    reward             | -2.0904434 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 6.57       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 102         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -155        |\n",
      "|    reward             | -0.44207352 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 37.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | -1.14     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -23.1     |\n",
      "|    reward             | 0.9330738 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 6.62      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 104      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | -0.135   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 92.9     |\n",
      "|    reward             | -7.76815 |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 7.51     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0.0581     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 96         |\n",
      "|    reward             | 0.27868846 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 7.2        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 105      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | -0.0792  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 220      |\n",
      "|    reward             | 5.968564 |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 34.2     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 75         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | -0.103     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -51.1      |\n",
      "|    reward             | -3.3952253 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 3.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -161      |\n",
      "|    reward             | 1.0586292 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 38.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 9.68      |\n",
      "|    reward             | 2.6588695 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 0.123     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 90          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0.00093     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -67.7       |\n",
      "|    reward             | -0.67736495 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 3.4         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -91.8      |\n",
      "|    reward             | 0.57783866 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 6.07       |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Now train with not tuned hyperaparameters\n",
    "#Default config.ddpg_PARAMS\n",
    "# non_tuned_model_ddpg = agent.get_model(\"ddpg\",model_kwargs = config.DDPG_PARAMS )\n",
    "# trained_ddpg = agent.train_model(model=non_tuned_model_ddpg, \n",
    "#                              tb_log_name='ddpg',\n",
    "#                              total_timesteps=50000)\n",
    "non_tuned_model_a2c = agent.get_model(\"a2c\", model_kwargs=config.A2C_PARAMS)\n",
    "trained_a2c = agent.train_model(model=non_tuned_model_a2c, tb_log_name='a2c', total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZbEYRQ1wBeC",
    "outputId": "c2aacb1f-9fc1-4816-c15a-da512df2b2df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
    "    model=trained_a2c, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFdB4YM3wh0m",
    "outputId": "e31ca771-bf0f-4f5c-d385-839dfa215450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return          0.246024\n",
      "Cumulative returns     0.365618\n",
      "Annual volatility      0.187158\n",
      "Sharpe ratio           1.271821\n",
      "Calmar ratio           2.602306\n",
      "Stability              0.921153\n",
      "Max drawdown          -0.094541\n",
      "Omega ratio            1.255015\n",
      "Sortino ratio          2.041262\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.146094\n",
      "Daily value at risk   -0.022635\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Backtesting for not tuned hyperparamters\n",
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
    "# perf_stats_all.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "12fxdvUZwi_W",
    "outputId": "56a25b27-dd6c-4620-acf1-76e68f252f32"
   },
   "outputs": [],
   "source": [
    "#You can see with trial, our sharpe ratio is increasing\n",
    "#Certainly you can afford more number of trials for further optimization\n",
    "from optuna.visualization import plot_optimization_history\n",
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "_TUF2GvAx6-k"
   },
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "2jkqeSUIyCT0",
    "outputId": "531590bb-e10c-4a96-b1d7-af57ff166030"
   },
   "outputs": [],
   "source": [
    "#Hyperparamters importance\n",
    "#Ent_coef is the most important\n",
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAD0MIAWukB9"
   },
   "source": [
    "## Further works\n",
    "\n",
    "1.   You can tune more critical hyperparameters\n",
    "2.   Multi-objective hyperparameter optimization using Optuna. Here we can maximize Sharpe and simultaneously minimize Volatility in our account value to tune our hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "_edBJqB8yEDr",
    "outputId": "2bf34851-7038-4341-b3cb-de7a85608c78"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
       "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
       "            <div id=\"21d54349-a5ac-4f06-adb0-e17b25a08512\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                \n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"21d54349-a5ac-4f06-adb0-e17b25a08512\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '21d54349-a5ac-4f06-adb0-e17b25a08512',\n",
       "                        [{\"mode\": \"lines\", \"name\": \"ddpg_study\", \"type\": \"scatter\", \"x\": [1.399998769842667, 1.4057362727350557, 1.4114737756274442, 1.417211278519833, 1.4229487814122215, 1.4286862843046102, 1.434423787196999, 1.4401612900893874, 1.4458987929817761, 1.4516362958741647, 1.4573737987665534, 1.463111301658942, 1.4688488045513306, 1.4745863074437193, 1.4803238103361078, 1.4860613132284965, 1.4917988161208853, 1.4975363190132738, 1.5032738219056625, 1.509011324798051, 1.5147488276904397, 1.5204863305828284, 1.526223833475217, 1.5319613363676057, 1.5376988392599942, 1.543436342152383, 1.5491738450447716, 1.5549113479371601, 1.5606488508295489, 1.5663863537219374, 1.572123856614326, 1.5778613595067146, 1.5835988623991033, 1.589336365291492, 1.5950738681838805, 1.6008113710762693, 1.606548873968658, 1.6122863768610465, 1.6180238797534352, 1.6237613826458237, 1.6294988855382124, 1.635236388430601, 1.6409738913229897, 1.6467113942153784, 1.6524488971077669, 1.6581864000001556, 1.6639239028925443, 1.6696614057849328, 1.6753989086773216, 1.68113641156971, 1.6868739144620988, 1.6926114173544873, 1.698348920246876, 1.7040864231392647, 1.7098239260316532, 1.715561428924042, 1.7212989318164307, 1.7270364347088192, 1.732773937601208, 1.7385114404935964, 1.7442489433859851, 1.7499864462783736, 1.7557239491707624, 1.761461452063151, 1.7671989549555396, 1.7729364578479283, 1.778673960740317, 1.7844114636327055, 1.7901489665250943, 1.7958864694174828, 1.8016239723098715, 1.80736147520226, 1.8130989780946487, 1.8188364809870374, 1.824573983879426, 1.8303114867718147, 1.8360489896642034, 1.8417864925565919, 1.8475239954489804, 1.853261498341369, 1.8589990012337578, 1.8647365041261463, 1.870474007018535, 1.8762115099109238, 1.8819490128033123, 1.887686515695701, 1.8934240185880897, 1.8991615214804782, 1.9048990243728667, 1.9106365272652555, 1.9163740301576442, 1.9221115330500327, 1.9278490359424214, 1.9335865388348101, 1.9393240417271986, 1.9450615446195874, 1.950799047511976, 1.9565365504043646, 1.962274053296753, 1.9680115561891418], \"y\": [0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.15384615384615385, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.23076923076923078, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.38461538461538464, 0.38461538461538464, 0.38461538461538464, 0.38461538461538464, 0.38461538461538464, 0.46153846153846156, 0.46153846153846156, 0.46153846153846156, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.5384615384615384, 0.6153846153846154, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.7692307692307693, 0.8461538461538461, 0.8461538461538461, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 1.0]}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Empirical Distribution Function Plot\"}, \"xaxis\": {\"title\": {\"text\": \"Objective Value\"}}, \"yaxis\": {\"range\": [0, 1], \"title\": {\"text\": \"Cumulative Probability\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('21d54349-a5ac-4f06-adb0-e17b25a08512');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                \n",
       "            </script>\n",
       "        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_edf(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXTgG1yvgeyq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FinRL_Hyperparameter tuning using Optuna",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "m_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
